---
title: AudioContext
slug: Web/API/AudioContext
tags:
  - API
translation_of: Web/API/AudioContext
---
<p>{{APIRef("Web Audio API")}}</p>

<p><code>AudioContext</code> インターフェイスは{{domxref("AudioNode")}}によって表現され、一緒にリンクした音声モジュールから作った音声処理グラフを表します。音声コンテキストはコンテキストを含むノードの作成と音声処理もしくはデコードの実行の両方を制御します。コンテキスト内部で全てのことが起こるので、何かをする前に <code>AudioContext</code> を作る必要があります。</p>

<p>{{InheritanceDiagram}}</p>

<h2 id="Constructor" name="Constructor">Constructor</h2>

<dl>
 <dt>{{domxref("AudioContext.AudioContext", "AudioContext()")}}</dt>
 <dd><code>AudioContext</code> オブジェクトを新しく作成し、返します。</dd>
</dl>

<h2 id="プロパティ">プロパティ</h2>

<p><em>親インターフェイス{{domxref("BaseAudioContext")}}からプロパティを継承します。</em></p>

<dl>
 <dt>{{domxref("AudioContext.baseLatency")}} {{readonlyinline}} {{experimental_inline}}</dt>
 <dd>{{domxref("AudioDestinationNode")}}から音声サブシステムまでの音声を渡す{{domxref("AudioContext")}}によって起きる処理レイテンシーの秒数を返します。</dd>
 <dt>{{domxref("AudioContext.outputLatency")}} {{readonlyinline}} {{experimental_inline}}</dt>
 <dd>現在の音声コンテキストの出力レイテンシーの見積もりを返します。</dd>
</dl>

<h2 id="メソッド">メソッド</h2>

<p>親インターフェイス{{domxref("<em>BaseAudioContext</em>")}} からメソッドを継承します。</p>

<dl>
 <dt>{{domxref("AudioContext.close()")}}</dt>
 <dd>任意のシステム音声リソースをリリースするために、音声コンテキストを閉じます。</dd>
 <dt>{{domxref("AudioContext.createMediaElementSource()")}}</dt>
 <dd>{{domxref("HTMLMediaElement")}}と関連付けられた{{domxref("MediaElementAudioSourceNode")}}を生成します。これは{{HTMLElement("video")}}要素もしくは{{HTMLElement("audio")}}要素からの再生や操作をするために使うことができます。</dd>
 <dt>{{domxref("AudioContext.createMediaStreamSource()")}}</dt>
 <dd>ローカルのコンピューターのマイクもしくは他のソースから来る音声ストリームを表現している{{domxref("MediaStream")}}と関連付けられた{{domxref("MediaStreamAudioSourceNode")}}を生成します。</dd>
 <dt>{{domxref("AudioContext.createMediaStreamDestination()")}}</dt>
 <dd>ローカルファイルに保存されたものかその他のコンピューターに送信された音声ストリームを表している{{domxref("MediaStream")}}と関連付けられた{{domxref("MediaStreamAudioDestinationNode")}}を生成します</dd>
 <dt>{{domxref("AudioContext.createMediaStreamTrackSource()")}}</dt>
 <dd>メディアストリームトラックを表している{{domxref("MediaStream")}}と関連づけられた{{domxref("MediaStreamTrackAudioSourceNode")}}を生成します。</dd>
 <dt>{{domxref("AudioContext.getOutputTimestamp()")}}</dt>
 <dd>二つの関連づけられたコンテキストの音声ストリームの位置の値を含んでいる <code>AudioTimestamp</code> オブジェクトを新しく返します。</dd>
 <dt>{{domxref("AudioContext.suspend()")}}</dt>
 <dd>一時的に音声ハードウェアアクセスを停止し、プロセスの CPU/バッテリー使用を減らすために、音声コンテキストの時間の進行を中断します。</dd>
 <dt>
 <h3 id="非推奨メソッド">非推奨メソッド</h3>
 </dt>
 <dt>{{domxref("AudioContext.resume()")}}</dt>
 <dd>あらかじめ中断させられた音声コンテキストの時間の進行を返します。</dd>
 <dt>
 <p>注意: <code>resume()</code> メソッドはまだ利用することができます。このメソッドは{{domxref("BaseAudioContext")}}インターフェイス({{domxref("BaseAudioContext.resume()")}}を見てください)上で現在定義されています。したがって、{{domxref("AudioContext")}}インターフェイスと{{domxref("OfflineAudioContext")}}インターフェイスの両方でアクセスすることができます。</p>
 </dt>
</dl>

<h2 id="例">例</h2>

<p>基本的な音声コンテキストの作成方法:</p>

<pre class="brush: js notranslate">var audioCtx = new AudioContext();</pre>

<p>クロスブラウザー対応版:</p>

<pre class="brush: js notranslate">var AudioContext = window.AudioContext || window.webkitAudioContext;
var audioCtx = new AudioContext();

var oscillatorNode = audioCtx.createOscillator();
var gainNode = audioCtx.createGain();
var finish = audioCtx.destination;
// etc.</pre>

<h2 id="仕様">仕様</h2>

<table class="standard-table">
 <tbody>
  <tr>
   <th scope="col">仕様書</th>
   <th scope="col">策定状況</th>
   <th scope="col">コメント</th>
  </tr>
  <tr>
   <td>{{SpecName('Web Audio API', '#AudioContext', 'AudioContext')}}</td>
   <td>{{Spec2('Web Audio API')}}</td>
   <td></td>
  </tr>
 </tbody>
</table>

<h2 id="ブラウザーの互換性">ブラウザーの互換性</h2>

<div>


<p>{{Compat("api.AudioContext")}}</p>
</div>

<h2 id="参考">参考</h2>

<ul style="margin-left: 40px;">
 <li><a href="/ja/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Web Audio API の使用</a></li>
 <li>{{domxref("OfflineAudioContext")}}</li>
</ul>
